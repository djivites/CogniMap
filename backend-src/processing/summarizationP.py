# -*- coding: utf-8 -*-
"""Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTDnitnPtZFc8HKqr5pz-YNaR2VMDXn7
"""


from datasets import load_dataset

df = load_dataset("knkarthick/dialogsum")

df

df['train'][1]['dialogue']

df['train'][1]['summary']

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import TrainingArguments, Trainer

tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-base")



"""**DATA EXPLORATION**

1.   DIMENSION



"""

import pandas as pd
train_df=pd.DataFrame(df['train'])
test_df=pd.DataFrame(df['test'])
print("shape of training:",train_df.shape)
print("shape of test:",test_df.shape)

"""2.)STATISTICAL SUMMARY"""

print(train_df.describe(include="all"))

"""3.)CHECK DUPLICATES

"""

# Duplicates only in summaries
print("Duplicate summaries:", train_df["summary"].duplicated().sum())

# Which summaries are duplicated
dup_summaries = train_df[train_df["summary"].duplicated(keep=False)]
print(dup_summaries[["summary", "dialogue"]].head(10))
print("Duplicate dialogues:", train_df["dialogue"].duplicated().sum())
print("Duplicate dialogue-summary pairs:", train_df.duplicated(subset=["dialogue","summary"]).sum())
print("Duplicate topics:", train_df["topic"].duplicated().sum())

"""4.)Missing values

"""

missing = train_df.isnull().sum()
missing_percent = (missing / len(train_df)) * 100
print("\nMissing Values per Column:")
print(pd.DataFrame({"Missing": missing, "Percentage": missing_percent}))

"""5.)Descriptive Analysis"""

print("\nDescriptive Analysis of Dialogue Length:")
train_df["dialogue_length"] = train_df["dialogue"].apply(lambda x: len(x.split()))
print(train_df["dialogue_length"].describe())

"""DATA PREPROCESSING

1.   REMOVING UNWANTED COLUMNS





"""

train_df = train_df.drop(columns=["id"],errors="ignore")
test_df = test_df.drop(columns=["id"],errors="ignore")
train_df = train_df.drop(columns=["topic"],errors="ignore")
test_df = test_df.drop(columns=["topic"],errors="ignore")
train_df = train_df.drop(columns=["dialogue_length"],errors="ignore")
test_df = test_df.drop(columns=["dialogue_length"],errors="ignore")
train_df.head()

""" 3. Clean text (HTML, URLs, junk chars, spaces)

"""

import re
def clean_text(text):
    text = re.sub(r"<.*?>", "", text)                       # remove HTML tags
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)     # remove URLs
    text = re.sub(r"[^a-zA-Z0-9.,!?;:'\"()\-\s]", "", text) # remove junk chars
    text = re.sub(r"\s+", " ", text).strip()                # normalize spaces
    return text

train_df["dialogue"] = train_df["dialogue"].apply(clean_text)
train_df["summary"] = train_df["summary"].apply(clean_text)
test_df["dialogue"] = test_df["dialogue"].apply(clean_text)
test_df["summary"] = test_df["summary"].apply(clean_text)

"""**Tokenization**

1.Convert columns to Python lists



"""

train_dialogues = train_df["dialogue"].tolist()
train_summaries = train_df["summary"].tolist()

test_dialogues = test_df["dialogue"].tolist()
test_summaries = test_df["summary"].tolist()

"""2) Tokenize source (dialogues) — batch encode, returns dict with input_ids & attention_mask"""

train_source_encodings = tokenizer(
    train_dialogues,
    truncation=True,
    padding="max_length",
    max_length=512
)
# train_source_encodings is a dict: {"input_ids": [...], "attention_mask": [...]}

test_source_encodings = tokenizer(
    test_dialogues,
    truncation=True,
    padding="max_length",
    max_length=512
)

"""3) Tokenize targets (summaries) using tokenizer.as_target_tokenizer()

"""

#    This ensures target tokenization follows seq2seq conventions
with tokenizer.as_target_tokenizer():
    train_target_encodings = tokenizer(
        train_summaries,
        truncation=True,
        padding="max_length",
        max_length=128
    )
    test_target_encodings = tokenizer(
        test_summaries,
        truncation=True,
        padding="max_length",
        max_length=128
    )
# train_target_encodings is a dict with "input_ids" for labels

"""4.Convert pad_token_id in target input_ids to -100 (so loss ignores padding)"""

#    We'll create a new `labels` list where every pad token becomes -100
pad_id = tokenizer.pad_token_id

train_labels = []
for label_seq in train_target_encodings["input_ids"]:
    # map pad -> -100
    mapped = [(tok if tok != pad_id else -100) for tok in label_seq]
    train_labels.append(mapped)

test_labels = []
for label_seq in test_target_encodings["input_ids"]:
    mapped = [(tok if tok != pad_id else -100) for tok in label_seq]
    test_labels.append(mapped)

"""5) Assemble final encodings dictionaries"""

train_encodings = {
    "input_ids": train_source_encodings["input_ids"],
    "attention_mask": train_source_encodings["attention_mask"],
    "labels": train_labels
}

test_encodings = {
    "input_ids": test_source_encodings["input_ids"],
    "attention_mask": test_source_encodings["attention_mask"],
    "labels": test_labels
}

print("Train examples:", len(train_encodings["input_ids"]))
print("Test examples:", len(test_encodings["input_ids"]))

print("Example input_ids length:", len(train_encodings["input_ids"][0]))
print("Example attention_mask length:", len(train_encodings["attention_mask"][0]))
print("Example labels length:", len(train_encodings["labels"][0]))
print("First 10 label tokens (showing -100 for pads):", train_encodings["labels"][8][:100])

import torch
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
    print("CUDA runtime version:", torch.version.cuda)
    print("cuDNN version:", torch.backends.cudnn.version())

"""**FINE TUNING THE MODEL**"""

import torch

class Seq2SeqDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

train_dataset = Seq2SeqDataset(train_encodings)
test_dataset = Seq2SeqDataset(test_encodings)

training_args = TrainingArguments( output_dir="/content",
per_device_train_batch_size=8,
num_train_epochs=2,
remove_unused_columns=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)
trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()

# Print evaluation results
print(eval_results)

"""#### SAVING THE MODEL"""

trainer.save_model("final_model2")
tokenizer.save_pretrained("final_mode2l")

"""**Example For Summarization**"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_dir = "final_model2"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)


# Function to summarize a blog post
def summarize(blog_post,length):
    # Tokenize the input blog post
    model_dir = "final_model"
    tokenizer = AutoTokenizer.from_pretrained(model_dir)       
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
    inputs = tokenizer(
        blog_post,
        max_length=1024,   # matches your training input length
        truncation=True,
        return_tensors="pt"
    )

    # Generate the summary
    summary_ids = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=length,   # matches your target length
        min_length=70,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )

    # Decode the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Example blog post
blog_post = """In 2018, 23 days after Thanos erased half of all life in the universe,[a] Captainmarvel rescues Tony Stark and Nebula from deep space. They reunite with the remaining Avengers—Bruce Banner, Steve Rogers, Thor, Natasha Romanoff, and James Rhodes—and Rocket on Earth. Locating Thanos on an uninhabited planet, they plan to use the Infinity Stones to reverse his actions but find that Thanos has destroyed them. Enraged, Thor decapitates Thanos.

Five years later, Scott Lang escapes from the Quantum Realm.[b] Reaching the Avengers Compound, he explains that he experienced only five hours while trapped. Theorizing that the Quantum Realm allows time travel, they ask a reluctant Stark to help them retrieve the Stones from the past to reverse Thanos's present actions. Stark, Rocket, and Banner, who has merged his intelligence with the Hulk's strength, build a time machine. Banner notes that altering the past does not affect the present; any changes create alternate realities. Banner and Rocket travel to Norway, where they visit the Asgardian refugee settlement of New Asgard and recruit an overweight, despondent Thor. In Tokyo, Romanoff recruits Clint Barton, who became a vigilante after the erasure of his family.

Banner, Lang, Rogers, and Stark time-travel to New York City during Loki's attack in 2012.[c] At the Sanctum Sanctorum, Banner convinces the Ancient One to give him the Time Stone after promising to return the Infinity Stones to their proper points in time. At Stark Tower, Rogers retrieves the Mind Stone from Hydra sleeper agents; Stark and Lang's attempt to steal the Space Stone fails, allowing 2012-Loki to escape with it. Rogers and Stark travel to Camp Lehigh in 1970, where Stark obtains an earlier version of the Space Stone and encounters his father, Howard. Rogers steals Pym Particles from Hank Pym to return to the present and sees his lost love, Peggy Carter.
Rocket and Thor travel to Asgard in 2013;[d] Rocket extracts the Reality Stone from Jane Foster. Thor is encouraged by his mother, Frigga, and retrieves his old hammer, Mjolnir. Barton, Romanoff, Nebula, and Rhodes travel to 2014; Nebula and Rhodes go to Morag and steal the Power Stone before Peter Quill can,[e] while Barton and Romanoff travel to Vormir. The Soul Stone's keeper, Red Skull, says that it can only be acquired by sacrificing a loved one. Romanoff sacrifices herself, allowing Barton to get the Stone. Rhodes and Nebula attempt to return to their own time, but Nebula is incapacitated when her cybernetic implants link with her past self; this allows 2014-Thanos to learn about his future self's success and the Avengers' attempt to undo it. 2014-Thanos sends 2014-Nebula forward in time to prepare for his arrival.
Reuniting in the present, the Avengers place the Stones into a gauntlet that Stark, Banner, and Rocket have built. Banner, who has the most resistance to their radiation, uses the gauntlet to undo Thanos's disintegrations. Meanwhile, 2014-Nebula (impersonating her future self) uses the time machine to transport 2014-Thanos and his warship to the present and destroys the Avengers Compound. Present-day Nebula convinces 2014-Gamora to betray Thanos, but is unable to convince 2014-Nebula and kills her. Thanos overpowers Stark, Thor and a Mjolnir-wielding Rogers; he summons his army to retrieve the Stones, intent on using them to destroy the universe and create a new one. A restored Stephen Strange arrives with other sorcerers, the restored Avengers and Guardians of the Galaxy, the Ravagers, and the armies of Wakanda and Asgard to fight Thanos's army. Danvers also arrives and destroys Thanos's warship, but Thanos overpowers her and seizes the gauntlet. Stark steals the Stones and uses them to disintegrate Thanos and his army, sacrificing himself.

After Stark's funeral, Thor appoints Valkyrie as the new king of New Asgard and joins the Guardians. Rogers returns the Stones and Mjolnir to their proper timelines and remains in the past to live with Carter. In the present, an elderly Rogers passes his shield to Sam Wilson."""



# Get the summary
summary = summarize(blog_post)
print("Summary:", summary)
